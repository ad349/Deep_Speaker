{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from models.bioid_cnn_lstm import inference_val\n",
    "from utils import get_filterbanks\n",
    "import bioid\n",
    "import lfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../test/*/*.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folders = glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(folders, columns=['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../test/002167_librispeech/002167_librispeech@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../test/002167_librispeech/002167_librispeech@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../test/002167_librispeech/002167_librispeech@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../test/002167_librispeech/002167_librispeech@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../test/012980_nist2010/012980_nist2010@10837_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename\n",
       "0  ../test/002167_librispeech/002167_librispeech@...\n",
       "1  ../test/002167_librispeech/002167_librispeech@...\n",
       "2  ../test/002167_librispeech/002167_librispeech@...\n",
       "3  ../test/002167_librispeech/002167_librispeech@...\n",
       "4  ../test/012980_nist2010/012980_nist2010@10837_..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['speakerid'] = df.filename.apply(lambda x: x.split('/')[-1].split('@')[0])\n",
    "df['count'] = df.groupby(df['speakerid'])['speakerid'].transform('count')\n",
    "df = df[df['count'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = df.sample(100).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "speaker_ids = df['speakerid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imposter_id = speaker_ids[:len(speaker_ids)/2]\n",
    "user_id = speaker_ids[len(speaker_ids)/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_enrollments=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    # saver=tf.train.Saver()\n",
    "    with tf.Graph().as_default():\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9, allow_growth=True)\n",
    "        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False, allow_soft_placement=True))\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        with sess.as_default():\n",
    "            with tf.device('/gpu:0'):\n",
    "                bioid.load_model_eval(sess, './models/bioid/20180124-173845/')\n",
    "                waves_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "                embeddings = tf.get_default_graph().get_tensor_by_name(\"L2_Norm/embedding:0\")\n",
    "                phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "                batch_size_placeholder = tf.get_default_graph().get_tensor_by_name(\"batch_size:0\")\n",
    "                enrollment_df = pd.DataFrame()\n",
    "                user_df = pd.DataFrame()\n",
    "                print('1')\n",
    "                for i in user_id:\n",
    "                    user_temp = df[df['speakerid']==i].reset_index(drop=True)\n",
    "                    enrollment_df = pd.concat([enrollment_df,user_temp[:num_enrollments]],axis=0)\n",
    "                    user_df = pd.concat([user_df,user_temp[num_enrollments:]],axis=0)\n",
    "                enrollment_df = enrollment_df.reset_index(drop=True)\n",
    "                user_df = user_df.reset_index(drop=True)\n",
    "                imposter_df = pd.DataFrame()\n",
    "                print('2')\n",
    "                for i in imposter_id:\n",
    "                    imposter_temp = df[df['speakerid']==i].reset_index(drop=True)\n",
    "                    imposter_df = pd.concat([imposter_df,imposter_temp[:num_enrollments]],axis=0)\n",
    "                print('3')\n",
    "                imposter_df = imposter_df.reset_index(drop=True)\n",
    "                eval_xs_enrollment = enrollment_df['filename'].apply(lambda x: get_filterbanks(x))\n",
    "                eval_xs_user = user_df['filename'].apply(lambda x:get_filterbanks(x))\n",
    "                eval_xs_imposter = imposter_df['filename'].apply(lambda x:get_filterbanks(x))\n",
    "                embd_enrollment=[]\n",
    "                embd_user=[]\n",
    "                embd_imposter=[]\n",
    "                print('4')\n",
    "                print('Genrating Embeddings !!')\n",
    "                for eval_x in eval_xs_enrollment:\n",
    "                    eval_x = np.expand_dims(eval_x, axis=0)\n",
    "                    embd_enrollment.append(sess.run(embeddings,\n",
    "                                                    feed_dict={waves_placeholder:eval_x, \n",
    "                                                               phase_train_placeholder:False,\n",
    "                                                               batch_size_placeholder:1}))\n",
    "                for eval_x in eval_xs_user:\n",
    "                    eval_x = np.expand_dims(eval_x, axis=0)\n",
    "                    embd_user.append(sess.run(embeddings,\n",
    "                                              feed_dict={waves_placeholder:eval_x, \n",
    "                                                         phase_train_placeholder:False,\n",
    "                                                         batch_size_placeholder:1}))\n",
    "                for eval_x in eval_xs_imposter:\n",
    "                    eval_x = np.expand_dims(eval_x, axis=0)\n",
    "                    embd_imposter.append(sess.run(embeddings,\n",
    "                                                  feed_dict={waves_placeholder:eval_x, \n",
    "                                                             phase_train_placeholder:False,\n",
    "                                                             batch_size_placeholder:1}))\n",
    "                print('5')\n",
    "                enrollment_df['embedding']=embd_enrollment\n",
    "                user_df['embedding']=embd_user\n",
    "                imposter_df['embedding']=embd_imposter\n",
    "                sess.close()\n",
    "    return(enrollment_df ,user_df,imposter_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: ./models/bioid/20180124-173845/\n",
      "Metagraph file: model-20180124-173845.meta\n",
      "Checkpoint file: model-20180124-173845.ckpt-20541\n",
      "INFO:tensorflow:Restoring parameters from ./models/bioid/20180124-173845/model-20180124-173845.ckpt-20541\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/abhishek_dandona/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/abhishek_dandona/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/abhishek_dandona/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 376, in _fixed_getinnerframes\n",
      "    lines = ulinecache.getlines(file)[start:end]\n",
      "  File \"/home/abhishek_dandona/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/utils/ulinecache.py\", line 37, in getlines\n",
      "    return [l.decode(encoding, 'replace') for l in lines]\n",
      "  File \"/home/abhishek_dandona/anaconda2/envs/tensorflow/lib/python2.7/encodings/utf_8.py\", line 15, in decode\n",
      "    def decode(input, errors='strict'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/abhishek_dandona/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhishek_dandona/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1826\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhishek_dandona/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1411\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhishek_dandona/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1319\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m             )\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhishek_dandona/anaconda2/envs/tensorflow/lib/python2.7/site-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "enrollment_df ,user_df,imposter_df = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enrolled_speakers=[]\n",
    "for spk in enrollment_df['speakerid'].unique():\n",
    "    temp=enrollment_df[enrollment_df['speakerid']==spk]\n",
    "    temp_embd=sum(temp['embedding'])/float(len(temp))\n",
    "    enrolled_speakers.append([spk,temp_embd])\n",
    "enrollment_df=pd.DataFrame(enrolled_speakers,columns=['speakerid','embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trials=[]\n",
    "for spk in enrollment_df['speakerid'].unique():\n",
    "    temp=user_df[user_df['speakerid']==spk].reset_index(drop=True)\n",
    "    temp_enrolled=pd.DataFrame()\n",
    "    temp_enrolled['embedding_A']=[enrollment_df[enrollment_df['speakerid']==spk]['embedding'].reset_index(drop=True)[0]]*len(temp)\n",
    "    temp_enrolled['speaker_A']=[spk]*len(temp)\n",
    "    trials.append(pd.concat([temp_enrolled[['embedding_A','speaker_A']],temp[['speakerid','embedding']]],axis=1))\n",
    "    trials.append(pd.concat([temp_enrolled[['embedding_A','speaker_A']],imposter_df.sample(n=len(temp)).reset_index(drop=True)[['speakerid','embedding']]],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_trials=pd.concat(trials).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_score=[]\n",
    "target=[]\n",
    "for i in _trials.iterrows():\n",
    "    embddA=i[1][0][0].reshape(1,-1)\n",
    "    embddB=i[1][-1][0].reshape(1,-1)\n",
    "    cosine_score.append(cosine_similarity(embddA,embddB)[0][0])\n",
    "    target.append(i[1][1]==i[1][2])\n",
    "_trials['cosine_score']=cosine_score\n",
    "_trials['target']=target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eer_threshold=0.79114027661991715\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, threshold = roc_curve(_trials['target'].apply(lambda x:1 if x else 0),_trials['cosine_score'],pos_label=1)\n",
    "fnr = 1 - tpr\n",
    "eer_threshold = threshold[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "print('EER threshold ',eer_threshold)\n",
    "EER = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n",
    "print('EER ',EER)\n",
    "\n",
    "print('number of Trials : %d '%len(_trials['target']))\n",
    "print('number of binary correct predictions %d'%sum((_trials['target'])==(_trials['cosine_score']>eer_threshold)))\n",
    "auc=sum((_trials['target'])==(_trials['cosine_score']>eer_threshold))/float(len(_trials['target']))\n",
    "print('Binary classification AUC : %1.3f'%auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: ./models/bioid/20180124-173845/\n",
      "Metagraph file: model-20180124-173845.meta\n",
      "Checkpoint file: model-20180124-173845.ckpt-20541\n",
      "INFO:tensorflow:Restoring parameters from ./models/bioid/20180124-173845/model-20180124-173845.ckpt-20541\n"
     ]
    }
   ],
   "source": [
    "# with tf.Graph().as_default():\n",
    "#     with  tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         bioid.load_model('./models/bioid/20180124-173845/')\n",
    "#         x = [n.name for n in tf.get_default_graph().as_graph_def().node]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.7",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
